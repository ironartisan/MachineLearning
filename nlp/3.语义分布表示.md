# 3.语义分布表示

词语语义表示的思路有两类，
- 一类是将词语映射为语义空间中的向量，把词语表示成计算机可操作的向量形式；
  - Onehot表示
    - 具有数据稀疏，存在维度爆炸及无法表达词语关联关系等问题
  - 分布式表示
    - NNLM
      - N 元语言模型即表示这个词语与其前面的 N-1 个词语有关，即输入前 N-1 个词语，然后预测窗口中下一个词语的概率，模型的优化目标是使得预测概率最大似然化。
    - CBOW
      - 给定一个句子，通过目标词语的上下文来推测该中心词词向量
    - Skip-gram
      - 在给定中心词的情况下，预测该词上下文向量
    - Glove
      - 利用语料库的全局特征，即词语的共现频次矩阵，优化后的目标函数为对数线性函数，通过回归求解
    - FastText
      - 基于字符级别来对词语进行建模，将整篇文档的词及 n-gram 向量作为输入，叠加之后取平均得到文档向量，再使用文档向量做softmax 多分类
- 另一类是将词语使用一个空间分布来表示，用概率分布来描述词语的语义。单词由一个完整的概率分布来表示，而不是一个确定性的点向量


中文词嵌入方法：
- 利用字符构成来学习词向量模型
  - CWE
    - 该模型的中心思想是假设一个词的语义是由该词的特定语义与构成该词的汉字的含义结合而成
  - SCWE
    - 考虑到词语中的字对词语的贡献度不一样，根据字与词的相似度赋予不同的权重加入模型训练
  - MGE
    - 核心思想是将上下文表示为周围词语、周围字符和目标词语的偏旁部首的组合，更加细粒度地结合字符和部首来增强词的向量表示。
  - JWE
    - 联合学习词级、字符级和更加细粒度的子字符即组件信息来学习词嵌入
  - cw2vec 
    - 笔画的 n-gram 特征信息被用来学习词嵌入
- 利用字符的视觉信息来学习词语表示
  - GWE
    - 用卷积自编码器可以直接从图像中学习字符特征
  - VCWE
    - 基于视觉增强的词嵌入模型




## 参考资料

- 中文词语语义的高斯分布表示与学习模型研究_易洁