# 1.NLP介绍与词向量初步

第1课直接切入语言和词向量，讲解了自然语言处理的基本概念，文本表征的方法和演进，包括word2vec等核心方法，词向量的应用等。

- [自然语言与文字](##自然语言与词汇含义)
- word2vec介绍
- word2vec目标函数与梯度
- 算法优化基础
- word2vec构建的词向量模式

## 1. 自然语言与词汇含义
### 1.1 人类的语言与词汇含义

人类之所以比类人猿更“聪明”，是因为我们有语言，因此是一个人机网络，其中人类语言作为网络语言。人类语言具有信息功能和社会功能。

据估计，人类语言只有大约5000年的短暂历史。语言和写作是让人类变得强大的原因之一。它使知识能够在空间上传送到世界各地，并在时间上传送。

但是，相较于如今的互联网的传播速度而言，人类语言是一种缓慢的语言。然而，只需人类语言形式的几百位信息，就可以构建整个视觉场景。这就是自然语言如此迷人的原因。

### 1.2 我们如何表达一个词的意思？

我们如何表达一个词的含义呢？有如下一些方式：

- 用一个词、词组等表示的概念
- 一个人想用语言、符号等来表达的想法
- 表达在作品、艺术等方面的思想

理解意义的最普遍的语言方式(linguistic way)：语言符号与语言意义（想法、事情）的相互对应

denotational semantics：语义

`signifier(symbol) <=> signifier(idea or thing) `

### 1.3 如何在计算机里表达词的意义

要使用计算机处理文本词汇，一种处理方式是WordNet：即构建一个包含同义词集和上位词(“is a”关系)的列表的辞典。英文当中确实有这样一个 wordnet，我们在安装完NLTK工具库和下载数据包后可以使用，对应的 python 代码如下：

```python
from nltk.corpus import wordnet as wn
poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}
for synset in wn.synsets("good"):
        print("{}: {}".format(poses[synset.pos()], ", ".join([l.name() for l in synset.lemmas()])))
from nltk.corpus import wordnet as wn
panda = wn.synset("panda.n.01")
hyper = lambda s: s.hypernyms()
list(panda.closure(hyper))
```

```
noun: good
noun: good, goodness
noun: good, goodness
noun: commodity, trade_good, good
adj: good
adj (s): full, good
adj: good
adj (s): estimable, good, honorable, respectable
adj (s): beneficial, good
adj (s): good
adj (s): good, just, upright
adj (s): adept, expert, good, practiced, proficient, skillful, skilful
adj (s): good
adj (s): dear, good, near
adj (s): dependable, good, safe, secure
adj (s): good, right, ripe
adj (s): good, well
adj (s): effective, good, in_effect, in_force
adj (s): good
adj (s): good, serious
adj (s): good, sound
adj (s): good, salutary
adj (s): good, honest
adj (s): good, undecomposed, unspoiled, unspoilt
adj (s): good
adv: well, good
adv: thoroughly, soundly, good
[Synset('procyonid.n.01'),
 Synset('carnivore.n.01'),
 Synset('placental.n.01'),
 Synset('mammal.n.01'),
 Synset('vertebrate.n.01'),
 Synset('chordate.n.01'),
 Synset('animal.n.01'),
 Synset('organism.n.01'),
 Synset('living_thing.n.01'),
 Synset('whole.n.02'),
 Synset('object.n.01'),
 Synset('physical_entity.n.01'),
 Synset('entity.n.01')]

```
### 1.4 WordNet的问题

大家可以将WordNet视作1个专家经验总结出来的词汇表，但它存在一些问题：

- 忽略了词汇的细微差别
  - 例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的。
- 缺少单词的新含义
  - 难以持续更新！
  - 例如：wicked、badass、nifty、wizard、genius、ninja、bombast
- 因为是小部分专家构建的，有一定的主观性
- 构建与调整都需要很多的人力成本
- 无法定量计算出单词相似度

### 1.5 文本(词汇)的离散表征

在传统的自然语言处理中，我们会对文本做离散表征，把词语看作离散的符号：例如hotel、conference、motel等。

一种文本的离散表示形式是把单词表征为独热向量(one-hot vectors)的形式

独热向量：只有一个1，其余均为0的稀疏向量

在独热向量表示中，向量维度 = 词汇量(如500,000)，以下为一些独热向量编码过后的单词向量示例：

motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]

hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]

### 1.6 离散表征的问题

在上述的独热向量离散表征里，所有词向量是正交的，这是一个很大的问题。对于独热向量，没有关于相似性概念，并且向量维度过大。

对于上述问题有一些解决思路：
- 使用类似WordNet的工具中的列表，获得相似度，但会因不够完整而失败
- 通过大量数据学习词向量本身相似性，获得更精确的稠密词向量编码

### 1.7 基于上下文的词汇表征

近年来在深度学习中比较有效的方式是基于上下文的词汇表征。它的核心想法是：**一个单词的意思是由经常出现在它附近的单词给出的** “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)。
- 这是现代统计NLP最成功的理念之一
- 总体思路有点物以类聚，人以群分的感觉。

当一个单词$$w$$出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)
基于海量数据，使用$$w$$的许多上下文来构建$$w$$的表示

如图所示，banking的含义可以根据上下文的内容表征。

![20220823235217-2022-08-23-23-52-18](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220823235217-2022-08-23-23-52-18.png)

## 2. Word2vec介绍


