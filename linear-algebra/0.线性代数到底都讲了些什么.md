
# 线性代数：线性代数到底都讲了些什么？

概率统计关注的是随机变量及其概率分布，以及如何通过观测数据来推断这
些分布。可是，在解决很多问题的时候，我们不仅要关心单个变量之间的关系，还要进一步
研究多个变量之间的关系，最典型的例子就是基于多个特征的信息检索和机器学习。

在信息检索中，我们需要考虑多个关键词特征对最终相关性的影响，而在机器学习中，无论
是监督式还是非监督式学习，我们都需要考虑多个特征对模型拟合的影响。在研究多个变量
之间关系的时候，线性代数成为了解决这类问题的有力工具。

另一方面，在我们日常生活和工作中，很多问题都可以线性化，小到计算两个地点之间的距
离，大到计算互联网中全部网页的 PageRank。所以，为了使用编程来解决相应的问题，
我们也必须掌握一些必要的线性代数基础知识。因此，我会从线性代数的基本概念出发，结
合信息检索和机器学习领域的知识，详细讲解线性代数的运用。

## 向量和向量空间

标量（Scalar）只是一个单独的数字，而且不能表示方向。从计算机数据结构的角度来看，标量就是编程中最基本的变量。这个很好理解，你可以回想一下刚开始学习编程时接触到的标量类型的变量。


和标量对应的概念，就是线性代数中最常用、也最重要的概念，向量（Vector），也可以
叫作矢量。它代表一组数字，并且这些数字是有序排列的。我们用数据结构的视角来看，向
量可以用数组或者链表来表达。


用加粗的小写字母表示一个向量，例如 x ，而 $x_1, x_2, x_3, x_n$
等等，来表示向量中的每个元素，这里面的 n 就是向量的维。

![20220629000016-2022-06-29-00-00-17](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000016-2022-06-29-00-00-17.png)

向量和标量最大的区别在于，向量除了拥有数值的大小，还拥有方向。向量或者矢量中
的“向”和“矢”这两个字，都表明它们是有方向的。你可能会问，为什么这一串数字能表
示方向呢？

这是因为，如果我们把某个向量中的元素看作坐标轴上的坐标，那么这个向量就可以看作空
间中的一个点。以原点为起点，以向量代表的点为终点，就能形成一条有向直线。而这样的
处理其实已经给向量赋予了代数的含义，使得计算的过程中更加直观。

由于一个向量包含了很多个元素，因此我们自然地就可以把它运用在机器学习的领域。

由于物体的特征有很多维，因此我们可以使用向量来表示某个物体的特征。其中，向量的每个元素就代表一维特
征，而元素的值代表了相应特征的值，我们称这类向量为特征向量（Feature Vector）。

需要注意的是，这个特征向量和矩阵的特征向量（Eigenvector）是两码事。那么矩阵的特
征向量是什么意思呢？矩阵的几何意义是坐标的变换。如果一个矩阵存在特征向量和特征
值，那么这个矩阵的特征向量就表示了它在空间中最主要的运动方向。如果你对这几个概念
还不太理解，也不用担心，在介绍矩阵的时候，我会详细说说什么是矩阵的特征向量。

## 向量的运算

标量和向量之间可以进行运算，比如标量和向量相加或者相乘时，我们直接把标量和向量中
的每个元素相加或者相乘就行了，这个很好理解。可是，向量和向量之间的加法或乘法应该
如何进行呢？我们需要先定义向量空间。向量空间理论上的定义比较繁琐，不过二维或者三
维的坐标空间可以很好地帮助你来理解。这些空间主要有几个特性：

- 空间由无穷多个的位置点组成；
- 这些点之间存在相对的关系；
- 可以在空间中定义任意两点之间的长度，以及任意两个向量之间的角度；
- 这个空间的点可以进行移动

有了这些特点，我们就可以定义向量之间的加法、乘法（或点乘）、距离和夹角等等。
两个向量之间的加法，首先它们需要维度相同，然后是对应的元素相加。

![20220629000252-2022-06-29-00-02-52](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000252-2022-06-29-00-02-52.png)


所以说，向量的加法实际上就是把几何问题转化成了代数问题，然后用代数的方法实现了几
何的运算。我下面画了一张图，来解释二维空间里，两个向量的相加，看完你就能理解了。

![20220629000341-2022-06-29-00-03-41](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000341-2022-06-29-00-03-41.png)

在这张图中，有两个向量 x 和 y，它们的长度分别是 x’和 y’，它们的相加结果是 x+y，
这个结果所对应的点相当于 x 向量沿着 y 向量的方向移动 y’，或者是 y 向量沿着 x 向量
的方向移动 x’。

向量之间的乘法默认是点乘，向量 x 和 y 的点乘是这么定义的：

![20220629000414-2022-06-29-00-04-14](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000414-2022-06-29-00-04-14.png)

点乘的作用是把相乘的两个向量转换成了标量，它有具体的几何含义。我们会用点乘来计算
向量的长度以及两个向量间的夹角，所以一般情况下我们会默认向量间的乘法是点乘。至于
向量之间的夹角和距离，它们在向量空间模型（Vector Space Model）中发挥了重要的作
用。信息检索和机器学习等领域充分利用了向量空间模型，计算不同对象之间的相似程度。


## 矩阵的运算

矩阵由多个长度相等的向量组成，其中的每列或者每行就是一个向量。因此，我们把向量延
伸一下就能得到矩阵（Matrix）。

从数据结构的角度看，向量是一维数组，那矩阵就是一个二维数组。如果二维数组里绝大多
数元素都是 0 或者不存在的值，那么我们就称这个矩阵很稀疏（Sparse）。对于稀疏矩
阵，我们可以使用哈希表的链地址法来表示。所以，矩阵中的每个元素有两个索引。

我用加粗的斜体大写字母表示一个矩阵，例如 X ，而 $X_{12}, X_{22},..., X_{nm}$ 等等，表
示矩阵中的每个元素，而这里面的 n 和 m 分别表示矩阵的行维数和列维数。

我们换个角度来看，向量其实也是一种特殊的矩阵。如果一个矩阵是 n × m 维，那么一个
n × 1 的矩阵也可以称作一个 n 维列向量；而一个 1 × m 矩阵也称为一个 m 维行向量。

同样，我们也可以定义标量和矩阵之间的加法和乘法，我们只需要把标量和矩阵中的每个元
素相加或相乘就可以了。剩下的问题就是，矩阵和矩阵之间是如何进行加法和乘法的呢？矩
阵加法比较简单，只要保证参与操作的两个矩阵具有相同的行维度和列维度，我们就可以把
对应的元素两两相加。而乘法略微繁琐一些，如果写成公式就是这种形式：

![20220629000700-2022-06-29-00-07-00](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000700-2022-06-29-00-07-00.png)

其中，矩阵Z为矩阵X和Y的乘积， X是形状为 i x k 的矩阵，而Y是形状为 k × j
的矩阵。 X的列数 k 必须和 Y的行数 k 相等，两者才可以进行这样的乘法。

我们可以把这个过程看作矩阵 的行向量和矩阵 的列向量两两进行点乘，我这里画了
张图，你理解了这张图就不难记住这个公式了。

![20220629000801-2022-06-29-00-08-02](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000801-2022-06-29-00-08-02.png)

两个矩阵中对应元素进行相乘，这种操作也是存在的，我们称它为元素对应乘积，或者
Hadamard 乘积。但是这种乘法咱们用得比较少，所以你只要知道有这个概念就可以了。

除了加法和乘法，矩阵还有一些其他重要的操作，包括转置、求逆矩阵、求特征值和求奇异
值等等。

转置（Transposition）是指矩阵内的元素行索引和纵索引互换，例如$X_{ij}$就变为$X_{ji}$ ，
相应的，矩阵的形状由转置前的 n × m 变为转置后的 m × n。从几何的角度来说，矩阵的
转置就是原矩阵以对角线为轴进行翻转后的结果。下面这张图展示了矩阵X转置之后的矩阵X'：

![20220629000911-2022-06-29-00-09-12](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629000911-2022-06-29-00-09-12.png)

除了转置矩阵，另一个重要的概念是逆矩阵。为了理解逆矩阵或矩阵逆（Matrix
Inversion），我们首先要理解单位矩阵（Identity Matrix）。单位矩阵中，所有沿主对角
线的元素都是 1，而其他位置的所有元素都是 0。通常我们只考虑单位矩阵为方阵的情况，
也就是行数和列数相等，我们把它记做$I_n$， 表示维数。我这里给出一个$I_5$ 的示例。

![20220629001215-2022-06-29-00-12-15](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220629001215-2022-06-29-00-12-15.png)


如果有矩阵$X$ ，我们把它的逆矩阵记做$X^{-1}$ ，两者相乘的结果是单位矩阵，写成公式就
是这种形式：

$$X^{-1} X = I_{n}$$

特征值和奇异值的概念以及求解比较复杂了，从大体上来理解，它们可以帮助我们找到矩阵
最主要的特点。通过这些操作，我们就可以在机器学习算法中降低特征向量的维度，达到特
征选择和变换的目的。

## 参考链接

- <https://time.geekbang.org/column/intro/100021201?code=HQ8p4bnGHEU%2Fd4XnfdGYiu2eXpy2OBD6AYV0Z5DYde4%3D>


