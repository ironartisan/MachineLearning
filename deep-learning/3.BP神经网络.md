# BP神经网络

> BP(back propagation)神经网络是1986年由Rumelhart和McClelland为首的科学家提出的概念，是一种按照误差逆向传播算法训练的多层前馈神经网络，是应用最广泛的神经网络模型之一，本文介绍BP神经网络的原理并介绍实际应用。

## 背景
1943年，心理学家麦卡洛克等人提出了M-P（McCulloch-Pitts）模型，其本质为模拟人类大脑的神经元和工作原理，掀起了人工神经网络的新篇章。1986年，深度学习之父Geoffrey提出了一种适用于多层感知器的反向传播（Back Propagation，BP）算法。他创新性地增加了神经网络误差的反向传播，该算法可以不断优化神经元间的参数，直到输出的误差在合理范围内。


## 原理
BP网络是目前在人工智能领域中应用最多的人工神经网络之一。BP神经网络结构架构如图所示。主要由输入层、隐含层和输出层组成。隐藏层的选择会影响网络内部计算过程，进而影响模型的拟合度及预测结果的误差大小。

![20220820205503-2022-08-20-20-55-04](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220820205503-2022-08-20-20-55-04.png)

BP神经网络的工作流程为：首先输入层接收信息，并将信息传递给中间的隐藏层；各层通过激励函数对前一层的输出加权处理；最后汇聚到输出层，得到最终计算的结果。将输入结果与目标结果进行对比，根据计算误差的大小来反向调节各个连接的权重。


## 代码实践

以历史轨迹数据为例，使用BP神经网络进行预测未来航迹走向。模型代码如下：

```
import torch
import torch.nn as nn

input_dim = 3


class LinearModel(nn.Module):
    """
    LinearModel.
    """
    def __init__(
        self, seq_len=8, h_dim=64,  use_cuda=0,embedding_dim=64
    ):
        super(LinearModel, self).__init__()
        self.h_dim = h_dim

        self.layer1 = torch.nn.Linear(input_dim, h_dim)
        self.layer2 = torch.nn.Linear(h_dim, input_dim)

    def forward(self, obs_traj):
        """
        Inputs:
        - obs_traj: Tensor of shape (obs_len, batch, 3)
        Output:
        - final_h: Tensor of shape (self.num_layers, batch, self.h_dim)
        """
        # Encode observed Trajectory
        result = []

        batch = obs_traj.size(1)
        obs_traj_embedding = self.layer1(obs_traj.contiguous().view(-1, input_dim))
        obs_traj_embedding = obs_traj_embedding.view(
            -1, batch, self.h_dim
         )
      
        cur_pos = self.layer2(obs_traj_embedding.view(-1, self.h_dim))
        cur_pos = cur_pos.view(-1, batch, input_dim)

        return cur_pos

```

完整代码参见Github链接：https://github.com/ironartisan/trajectory-prediction/tree/master/linear
