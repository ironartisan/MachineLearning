# 循环神经网络RNN

> 循环神经网络（Recurrent Neural Network, RNN）是一类以序列数据为输入，在序列的演进方向进行递归且所有节点（循环单元）按链式连接的递归神经网络。

## 背景
BP神经网络适用于前一个输入与后一个输入没有直接关系的数据，仅能单独地处理输入数据，无法处理在时间上有前后关联的问题。但是对于一些时间序列数据，当前时刻的数据与历史时刻的数据存在一定的关联关系，因此，在使用神经网络处理序列数据的问题上，需要能挖掘某些随机变量随时间不断变化的规律。

1982年，美国学者Hopfield提出了具有递归计算和记忆能力的Hopfield网络模型。1986年，Jordan提出具有延时输入的Jordan网络，通过连接网络的每个隐含层节点和状态单元，并使用logistic函数作为激励函数。1990年，Jeffrey Elman通过简化Jordan Network，提出了Elman网络，在模型训练时引入反向传播算法，形成了早期最简单的RNN模型。


## 原理

RNN与BP神经网络的主要区别在于它的记忆功能，他们从先前的输入中获取信息进而影响当前的输入和输出。BP神经网络的输入和输出是相互独立的，但RNN的输出结果还取决于序列中的先验元素。RNN的另一个显著特点是权重共享。BP神经网络在每个节点具有不同的权重，但RNN在网络的每一层内都共享相同的权重参数。循环神经网络的基础结构如图2.4所示。

![RNN-2022-08-20-21-48-20](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/RNN-2022-08-20-21-48-20.png)

![1-2022-08-20-21-57-11](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/1-2022-08-20-21-57-11.png)

由上式可知，RNN本质上是循环的，因为它对数据的每个输入执行相同的功能，而当前输入的输出取决于过去的计算。生成输出结果后，将其复制并发送回循环神经网络。当作出决定的时候，它会考虑当前输入和它从先前输入中学到的输出。

RNN模型大小不随输入大小增加。RNN已经具备了对信息的记忆能力，但RNN并不能真正解决时序数据的处理问题，因为梯度消失和梯度爆炸导致RNN并不能够保留长距离的信息。



