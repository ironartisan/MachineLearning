# Word2vec

> 选自论文[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

## 目标
- n-gram介绍
- 什么是词向量？有什么种类，以及生成词向量的方式有哪些？
- 分布式词向量的优点
- 分析神经网络语言模型、CBOW和Skip-gram模型的数据流程图。
- 根据实验结果分析为什么论文中表4中skip-gram比CBOW的模型效果好。
- 搞懂代码的各部分组成及功能

## 词向量

### One-hot Representation 

NLP相关任务中最常见的第一步是创建一个词表库并把每个词顺序编号。这实际就是词表示方法中的 One-hot Representation，这种方法把每个词顺序编号，每个词就是一个很长的向量，向量的维度等于词表大小，只有对应位置上的数字为 1，其他都为 0。当然在实际应用中，一般采用稀疏编码存储，主要采用词的编号。 

这种表示方法一个最大的问题是无法捕捉词与词之间的相似度，就算是近义词也无法从词向量中看出任何关系。此外这种表示方法还容易发生维数灾难。

### Distributed Representation 

基本思想是通过训练将每个词映射成 K维实数向量（K 一般为模型中的超参数），通过词之间的距离（比如 cosine相似度、欧氏距离等）来判断它们之间的语义相似度。而word2vec使用的就是这种 Distributed representation的词向量表示方式。

## 统计语言模型

表示语言基本单位（一般为句子）的概率分布函数，一般语言模型可以使用各个词语条件概率的形式表示：

$$
p(s)=p\left(w_1^T\right)=p\left(w_1, w_2, \ldots, w_T\right)=\prod_{t=1}^T p\left(w_t \mid \text { Context }\right)
$$

其中 Context即为上下文，根据对 Context不同的划分方法，可以分为五大类：

1）上下文无关模型（Context=NULL） 

该模型仅仅考虑当前词本身的概率，不考虑该词所对应的上下文环境。这是一种最简单，易于实现，但没有多大实际应用价值的统计语言模型。

$$
p\left(w_t \mid \text { Context }\right)=p\left(w_t\right)=\frac{N_{w_t}}{N}
$$

不考虑任何上下文信息，仅仅依赖于训练文本中的词频统计。它是n-gram模型中当 n=1的特殊情形，所以有时也称作 Unigram Model（一元文法统计模型）。实际应用中，常被应用到一些商用语音识别系统中。

2）n-gram模型

n=1时，就是上面所说的上下文无关模型，这里 n-gram 一般认为是 N>=2是的上下文相关模型。当 n=2时，也称为 Bigram语言模型，直观的想，在自然语言中 “白色汽车”的概率比“白色飞翔”的概率要大很多，也就是 p(汽车|白色)> p(飞翔|白色)。n>2也类似，只是往前看 n-1个词而不是一个词。

一般 n-gram模型优化的目标是最大 log似然，即

$$
\prod_{t=1}^T p_t\left(w_t \mid w_{t-\mathrm{n}+1}, w_{t-\mathrm{n}+2}, \ldots, w_{t-1}\right) \log p_m\left(w_t \mid w_{t-\mathrm{n}+1}, w_{t-\mathrm{n}+2}, \ldots, w_{t-1}\right)
$$

n-gram模型的优点包含了前 N-1个词所能提供的全部信息，这些信息对当前词出现具有很强的约束力。同时因为只看 N-1个词而不是所有词也使得模型的效率较高。

n-gram语言模型也存在一些问题： 
1. n-gram语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。大部分研究或工作都是使用 Trigram，就算使用高阶的模型，其统计到的概率可信度就大打折扣，还有一些比较小的问题采用 Bigram。 
2. 这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词，如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可能经常出现。 
3. 训练语料里面有些 n元组没有出现过，其对应的条件概率就是 0，导致计算一整句话的概率为 0。解决这个问题有两种常用方法： 
- 方法一为平滑法。最简单的方法是把每个 n元组的出现次数加 1，那么原来出现 k次的某个 n元组就会记为 k+1次，原来出现 0次的 n元组就会记为出现 1次。这种也称为 Laplace平滑。当然还有很多更复杂的其他平滑方法，其本质都是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入先验方法的不同也就产生了很多不同的平滑方法。 
- 方法二是回退法。有点像决策树中的后剪枝方法，即如果 n元的概率不到，那就往上回退一步，用 n-1元的概率乘上一个权重来模拟

3）n-pos模型

n-gram的一种衍生模型。n-gram模型假定第 t个词出现概率条件依赖它前 N-1个词，而现实中很多词出现的概率是条件依赖于它前面词的语法功能的。n-pos模型就是基于这种假设的模型，它将词按照其语法功能进行分类 ，由这些词类决定下 一个词出现的概率 。这样的词类称为词性（Part-of-Speech，简称为 POS）。n-pos模型中的每个词的条件概率表示为

$$
p(s)=p\left(w_1^T\right)=p\left(w_1, w_2, \ldots, w_T\right)=\prod_{t=1}^T p\left(w_t \mid c\left(w_{t-\mathrm{n}+1)}, c\left(w_{t-\mathrm{n}+2}\right), \ldots, c\left(w_{t-1}\right)\right)\right.
$$

$$c$$ 为类别映射函数, 即把 $$T$$ 个词映射到 $$k$$ 个类别 $$(1=<K<=T)$$ 。实际上 $$n-P o s$$ 使用了一种聚类的思想, 使得原来 n-gram 中 $$w_{t-n+1}, w_{t-n+2}, \ldots, w_{t-1}$$ 中的可能为 $$T^{N-1}$$ 减少到 $$c\left(w_{t-\mathrm{n}+1)}, c\left(w_{t-\mathrm{n}+2}\right), \ldots, c\left(w_{t-1}\right)\right.$$ 中的 $$K^{N-1}$$, 同时这种减少还采用了语义有意义的类别。

4）基于决策树的语言模型

上面提到的上下文无关语言模型、n-gram语言模型、n-pos语言模型等等.都可以以统计决策树的形式表示出来。而统计决策树中每个结点的决策规则是一个上下文相关的问题。

基于决策树的语言模型优点是：分布数不是预先固定好的，而是根据训练预料库中的实际情况确定，更为灵活。缺点是：构造统计决策树的问题很困难，且时空开销很大。

5) 最大熵模型

基本思想是：对一个随机事件的概率分布进行预测时，在满足全部已知的条件下对未知的情况不做任何主观假设。从信息论的角度来说就是：在只掌握关于未知分布的部分知识时，应当选取符合这些知识但又能使得熵最大的概率分布。

6）自适应语言模型 
前面的模型概率分布都是预先从训练语料库中估算好的，属于静态语言模型。而自适应语言模型类似是 Online Learning的过程，即根据少量新数据动态调整模型，属于动态模型。在自然语言中，经常出现这样现象：某些在文本中通常很少出现的词，在某一局部文本中突然大量地出现。能够根据词在局部文本中出现的情况动态地调整语言模型中的概率分布数据的语言模型成为动态、自适应或者基于缓存的语言模型。通常的做法是将静态模型与动态模型通过参数融合到一起，这种混合模型可以有效地避免数据稀疏的问题。 

