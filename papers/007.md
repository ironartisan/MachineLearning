# 007

**英文名称：** Skip-Thought Vectors

**中文名称：** 跳跃思维句表示

**论文地址：** http://arxiv.org/abs/1506.06726

**期刊/时间：** NIPS 2015

## 前置知识

### 句表示简介

神经网络模型生成的词表示被称作词向量，这是一个低维的向量表示，已经成为了当前工 业界和学术界的通用文本表示方法。在自然语言处理的领域，有着很多句子级别的任务， 例如情感分析、句对匹配等，毫无疑问，句子级别的建模能够更为有效地改善这些任务的处理性能。**通过某种方式将句子编码为计算机可以处理的形式（向量），实现一次表示， 多领域应用，是自然语言表示中的重要任务。**

一次表示，多领域应用，以及预先训练，应用微调已经成为了工业届解决具体NLP任务的最佳思路，因为其可以充分利用无监督的数据来辅助有监督任务学习语言信息，往往也能取得很好的效果。

两种应用思路：
- 句表示训练完毕后不再调整，只在具体任务上训练分类器；
- 或者句表示的编码模型也随着下游任务进行训练，如BERT。

### 句表示发展历史

趋势：
- 让句子的语义更“准确”地编码到有限维的向量中
- 在向量子空间中保持句子的语义关系
- 更好地利用语言模型以及无监督上下文信息

分为以下几种：
- 基于词袋模型的句表示
  - One-hot编码
    - 又称独热编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。One-hot在特征提取上属于词袋模型（bag of words）。
  - 词表示加权
    - 将词向量进行加权得到稠密的句表示，一般遵循一个准则：越常见的词权重越小，常见的两种方式：
      - TF-IDF
      - 通过对词向量加权的方式
    - 该方法具有以下特点：
      - 简单快捷
      - 没有考虑语序
      - 没有合理地利用句子语义信息
- 基于上下文、有监督/无监督的句表示
- 基于上下文、语言模型、无监督的句表示


### 基于神经网络的句表示

基于语言模型的句表示：利用了无监督文本语料，利用了词与词共现信息，可大规模训练。
- 优点：利用无监督语料，成本低；语言模型学习语言知识
- 缺点：句子之间隐藏的语义联系；学习句表示，忽略句子间的信息是极为不合理的 

基于复述句对的句表示方法：语料可通过机器翻译大规模获得；建模句对关系
- 优点：建模了句对之间的相似与不相似关系
- 缺点：仅仅建模了相关性，忽略了句子间复杂的语义关系

在一段话中，句子与其上下句总是存在某种语义关系的，能否通过一个句子预测上下文的句子？

Skip-thought：如何利用大规模无监督语料建模句子间的关系？

## 摘要

- **问题是什么？**
- **我们要做什么？**
  - 提出了一种无监督学习通用分布式句子编码器的方法。
- **大概怎么做的**
  - 利用书籍中文本的连续性，我们训练一个编码器-解码器模型，试图重建一个编码段落的周围句子。
  - 因此，共享语义和句法属性的句子被映射到类似的矢量表示中。
  - 我们接下来介绍一种简单的词汇扩展方法，对训练中没有看到的词汇进行编码，使我们的词汇量扩大到一百万个。
- **实验效果**
  - 在训练完我们的模型后，我们用线性模型在8个任务上提取并评估我们的向量：语义相关、释义检测、图像-句子排名、问题类型分类和4个基准情感和主观性数据集。
  - 最终的结果是一个现成的编码器，它可以产生高度通用的句子表征，在实践中表现良好。

## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
  - 为单词的分布式组合语义开发学习算法，是语言理解和机器学习交叉领域的一个长期的开放性问题。
- **承。相关工作**
  - 近年来，已经开发了几种方法来学习将词向量映射到句子向量的构成算子，包括递归网络、递归网络、卷积网络和递归卷积方法等等。所有这些方法产生的句子表征都被传递到一个有监督的任务中，并依赖于一个类别标签，以便通过组成权重进行反向传播。

- **转。相关工作的不足和转机**
  - 上述方法学习高质量的句子表征，但只针对各自的任务进行调整。
  - 段落向量是上述模型的一个替代方案，因为它可以通过引入分布式句子指标作为神经语言模型的一部分来学习无监督的句子表示。缺点是在测试时，需要进行推理来计算一个新的矢量。
- **合。本文工作**
  - 从组合方法本身中抽象出来，考虑了一个可以适用于任何组合操作者的替代损失函数。
  - 以词向量学习为灵感，提出了一个目标函数，将 skip-gram 模型抽象到句子层面。也就是说，我们不是用一个词来预测它周围的语境，而是对一个句子进行编码来预测它周围的句子。因此，任何组合运算符都可以被替换为句子编码器，只有目标函数会被修改
  - 在8个任务上表现SOTA。在实验中，我们提取了skip-thought的向量，并训练线性模型来直接评估表征，而不需要任何额外的微调。
  - 一个困难是能够构建一个足够大的单词词汇来编码任意的句子。例如，维基百科文章中的一个句子可能包含极不可能出现在我们图书词汇中的名词。我们通过学习一种将单词表征从一个模型转移到另一个模型的映射来解决这个问题。利用用连续词包模型学习的预训练的word2vec表征，我们学习从word2vec空间的一个词到编码器的词汇空间的一个词的线性映射。该映射是利用词汇表之间共享的所有单词学习的。经过训练，任何出现在word2vec中的词都可以在编码器的词嵌入空间中得到一个矢量。



## 相关工作

**主要介绍背景知识。**

## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**
  ![20220930143700-2022-09-30-14-37-00](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220930143700-2022-09-30-14-37-00.png)
  - 模型使用编码器解码器架构。使用了一个带有GRU激活的RNN编码器和一个带有条件GRU的RNN解码器
  - 一个编码器将单词映射到一个句子矢量，一个解码器用来生成周围的句子。
  - 损失函数：
    - 通过前（后）句子中t前面的词计算t位置的词。
$$
\sum_t \log P\left(w_{i+1}^t \mid w_{i+1}^{<t}, \mathbf{h}_i\right)+\sum_t \log P\left(w_{i-1}^t \mid w_{i-1}^{<t}, \mathbf{h}_i\right)
$$

## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**


## 讨论与总结



