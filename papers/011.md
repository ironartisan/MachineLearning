# 011

**英文名称：** Language Models are Few-Shot Learners


**论文地址：** https://arxiv.org/pdf/2005.14165.pdf

**期刊/时间：** NeurIPS 2020

## 前置知识

## 摘要

- **问题是什么？**
- **我们要做什么？**
- **大概怎么做的**
- **实验效果**

最近的工作表明，在许多NLP任务和基准上，通过对大型文本语料库进行预训练，然后对特定的任务进行微调，可以获得巨大的收益。虽然在结构上通常是任务无关的，但这种方法仍然需要特定任务的微调数据集，包括几千或几万个例子。相比之下，人类通常只需从几个例子或简单的指令中就能完成一项新的语言任务，而目前的NLP系统在很大程度上仍难以做到这一点。

在这里，我们扩大语言模型的规模，大大改善了与任务无关的、少量的性能，有时甚至达到了与之前最先进的微调方法的竞争力。

具体来说，我们训练了GPT-3，一个具有1750亿个参数的自回归语言模型，比以前的任何非稀疏语言模型多出10倍，并测试了它在少样本情况下的表现。对于所有的任务，GPT-3的应用没有任何梯度更新或微调，纯粹通过与模型的文本互动来指定任务和少量演示。GPT-3在许多NLP数据集上取得了强大的性能，包括翻译、回答问题和cloze任务，以及一些需要即时推理或领域适应的任务，如解读单词、在句子中使用一个新词或进行3位数的算术。同时，我们也发现了一些数据集，在这些数据集中，GPT-3的小样本学习仍然很困难，还有一些数据集，GPT-3面临着与大型网络语料库训练有关的方法学问题。最后，我们发现，GPT-3可以生成人类评价者难以区分的新闻文章样本。我们讨论了这一发现和GPT-3总体上的更广泛的社会影响。

## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
- **承。相关工作**
- **转。相关工作的不足和转机**
- **合。本文工作**

近年来，在NLP系统中出现了预训练语言表征的趋势，并以越来越灵活和与任务无关的方式应用于下游转移。首先，使用词向量学习单层表征[MCCD13, PSM14]，并反馈给特定的任务架构，然后使用具有多层表征和上下文状态的RNN来形成更强大的表征[DL15, MBXS17, PNZtY18]（尽管仍然适用于特定的任务架构），最近，预训练的递归或transformer语言模型[VSP+17]被直接微调，完全消除对特定任务架构的需求[RNSS18, DCLT18, HR18]。

这最后一种范式在许多具有挑战性的NLP任务上取得了实质性的进展，如阅读理解、问题回答、文本嵌套等，并在新的架构和算法的基础上继续推进[RSR+19, LOG+19, YDY+19, LCG+19]。**然而，这种方法的一个主要限制是，虽然架构是任务无关的，但仍然需要特定任务的数据集和特定任务的微调：要在一个期望的任务上实现强大的性能，通常需要在该任务特定的数千到数十万个例子的数据集中进行微调**。消除这一限制是可取的，原因有几个。

首先，从实用的角度来看，每一项新的任务都需要一个大型的标记实例数据集，这限制了语言模型的适用性。因为存在着非常广泛的可能有用的语言任务，包括从纠正语法，到生成抽象概念的例子，再到批评一个短篇故事。对于许多这样的任务来说，收集一个大型的监督训练数据集是很困难的，尤其是当这个过程对于每个新任务必须为重复时。

其次，利用训练数据中的虚假关联的潜力从根本上说是随着模型的表现力和训练分布的狭窄程度而增长的。这可能会给预训练加微调范式带来问题，即模型被设计得很大，以便在预训练期间吸收信息，但随后在非常狭窄的任务分布中进行微调。例如，[HLW+20]观察到较大的模型不一定在分布外有更好的泛化。有证据表明，在这种范式下实现的泛化可能很差，因为模型对训练分布过于具体，在分布外的泛化效果不好[YdC+19, MPL19]。因此，微调模型在特定基准上的表现，即使名义上是人类水平，也可能夸大了基础任务的实际表现[GSL+18, NK19]。

第三，人类不需要大量的监督数据集来学习大多数语言任务--自然语言的简短指令（例如 "请告诉我这个句子描述的是快乐的事情还是悲伤的事情"）或最多极少的示范（例如 "这里有两个人们表现勇敢的例子；请给出第三个勇敢的例子"）往往就是足以使人类在执行一项新任务时至少达到合理的能力水平。除了指出我们目前的NLP技术在概念上的局限性外，这种适应性还有实际的好处--它允许人类在许多任务和技能之间无缝混合或切换，例如在漫长的对话中进行加法。为了发挥广泛的作用，我们希望有一天我们的NLP系统也能有这样的流畅性和通用性。

解决这些问题的一个潜在途径是元学习1--在语言模型的背景下，这意味着模型在训练时发展出一套广泛的技能和模式识别能力，然后在推理时使用这些能力来快速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC+19]试图通过我们所说的 "语境学习 "来做到这一点，使用预训练的语言模型的文本输入作为任务规范的形式：该模型以自然语言指令和/或任务的几个示范为条件，然后期望通过预测接下来的内容来完成任务的进一步实例。

![20221105165446-2022-11-05-16-54-48](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20221105165446-2022-11-05-16-54-48.png)

虽然它已经显示出一些初步的希望，但这种方法所取得的结果仍然远远不如微调--例如[RWC+19]在自然问题上只取得了4%的成绩，甚至其55 F1 CoQa的结果现在也比技术水平落后35分以上。元学习显然需要大量的改进，以便作为解决语言任务的实用方法。

语言建模的另一个最新趋势可能提供了一个前进的方向。近年来，Transformer语言模型的容量大幅增加，从1亿个参数[RNSS18]，到3亿个参数[DCLT18]，到15亿个参数[RWC+19]，到80亿个参数[SPP+19]，110亿个参数[RSR+19]，最后到170亿个参数[Tur20]。每一次增加都带来了文本合成和/或下游NLP任务的改进，而且有证据表明，与许多下游任务有很好关联的对数损失，随着规模的扩大也有平滑的改进趋势[KMH+20]。由于语境学习涉及到在模型的参数范围内吸收许多技能和任务，因此，语境学习能力可能会随着规模的扩大而显示出类似的强劲收益，这是可信的。

在本文中，我们通过训练一个1750亿个参数的自回归语言模型（我们称之为GPT-3）来测试这一假设，并测量其语境学习能力。具体来说，我们在二十多个NLP数据集上评估GPT-3，以及几个旨在测试快速适应不太可能直接包含在训练集中的任务的新任务。对于每个任务，我们在3种条件下评估GPT-3：(a) "few-shot学习"，或语境学习，我们允许尽可能多的演示，以适应模型的上下文窗口（通常是10至100），(b) "One-shot学习"，我们只允许一次演示，和(c) "zero-shot"学习，不允许演示，只给模型一个自然语言的指令。原则上，GPT-3也可以在传统的微调设置中进行评估，但我们将此留给未来的工作。

图1.2说明了我们研究的条件，并显示了对一个简单任务的few-shot学习，该任务要求模型从一个词中去除不相干的符号。模型的性能随着自然语言任务描述的增加而提高，也随着模型上下文中的例子数量K的增加而提高。虽然这种情况下的结果特别引人注目，但模型规模和上下文中的例子数量的一般趋势在我们研究的大多数任务中都适用。我们强调，这些 "学习 "曲线不涉及梯度更新或微调，只是作为条件的示范数量增加。

![20221105172657-2022-11-05-17-26-57](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20221105172657-2022-11-05-17-26-57.png)

总的来说，在NLP任务中，GPT-3在zero-shot和one-shot设置中取得了令人满意的结果，在few-shot设置中有时与最先进的技术竞争，甚至有时超过最先进的技术(尽管最先进技术是由微调的模型保持的)。例如，GPT-3在zero-shot条件下CoQA达到81.5 F1，在one-shot条件下CoQA达到84.0 F1，在few-shot条件下达到85.0 F1。类似地，GPT-3在TriviaQA上的zero-shot命中率为64.3%，在one-shot命中率为68.0%，在few-shot命中率为71.2%，相对于在相同设置下运行的微调模型，后者是最先进的。

与此同时，我们也发现了一些任务，即使在GPT-3的规模下，few-shot的性能挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优缺点(包括这些局限性)进行广泛的描述，我们希望促进对语言模型中的few-shot学习的研究，并让人们注意到最需要进步的地方。

与此同时，我们也发现了一些任务，即使在GPT-3的规模下，few-shot的性能挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优缺点(包括这些局限性)进行广泛的描述，我们希望促进对语言模型中的few-shot学习的研究，并让人们注意到最需要进步的地方。

在图1.3中可以看到总体结果的启发式意义，它聚集了各种任务(尽管它本身不应该被视为严格或有意义的基准)。我们还对“数据污染”进行了系统的研究，这是一个日益严重的问题，当训练高容量模型时，如Common Crawl数据集，它可能包含来自测试数据集的内容，只是因为这样的内容经常存在于web上。在这篇论文中，我们开发了测量数据污染和量化其扭曲影响的系统工具。尽管我们发现数据污染对GPT-3在大多数数据集上的性能影响极小，但我们确实发现一些数据集上的结果可能是夸大的，我们要么不报告这些数据集上的结果，要么根据严重程度用星号标记它们。

除了以上所有这些，我们还训练了一系列较小的模型(从1.25亿个参数到130亿个参数)，以便比较它们在zero, one 及few-shot设置下的性能。总的来说，对于大多数任务，我们发现在所有三种设置中模型容量的伸缩相对平稳;一个值得注意的模式是，zero, one 及few-shot表现之间的差距通常随着模型容量的增加而增大，这可能表明较大的模型是更熟练的元学习者。

最后，鉴于GPT-3显示的广泛能力，我们讨论了对偏见、公平和更广泛的社会影响的关注，并试图对GPT-3在这方面的特点进行初步分析。


## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**

我们的基本预训练方法（包括模型，数据和训练）与gpt2中描述的过程相似，模型尺寸，数据集大小和多样性以及训练时间的扩展相对简单。我们在上下文学习中的使用也类似于gpt2，但是在这项工作中，我们系统地探索了在上下文中进行学习的不同设置。因此，我们从明确定义和对比将要评估GPT-3或原则上可以评估GPT-3的不同设置开始本节。这些设置可以看作取决于它们倾向于依赖多少特定于任务的数据。具体而言，我们可以至少识别四个点。


![20230109212947](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20230109212947.png)

Fine-tuing是近年来最常用的方法，它涉及通过在特定于所需任务的监督数据集上进行训练来更新预训练模型的权重。通常使用成千上万的标记示例。微调的主要优点是在许多基准上均具有出色的性能。主要缺点是，每个任务都需要一个新的大型数据集，存在泛化分布不佳的潜在可能性，以及利用训练数据的虚假特征，这可能会导致与人类绩效的不公平比较。在这项工作中，我们不会微调GPT-3，因为我们专注于与任务无关的性能，但原则上可以微调GPT-3，这是未来工作的有希望的方向。

Few-Shot是我们将在本工作中使用的术语，是指这样的设置，在该设置中，在推理时给模型一些任务演示作为条件，但不允许权重更新。如图2.1所示，典型数据集一个示例具有上下文和所需的完成度（例如，英语句子和法语翻译），并通过给出K个上下文和完成度的示例，再给出一个上下文的最终示例来演示少量例子，并期望模型完成。通常，将范围设置为10到100，因为这可以在模型的上下文窗口中容纳多少示例（nctx = 2048）。Few-Shot的主要优点是大大减少了对特定于任务的数据的需求，并减少了从大型但狭窄的微调数据集中学习过窄分布的潜力。主要缺点是，迄今为止，此方法的结果比最新的微调模型差很多。而且，仍然需要少量的任务特定数据。

One-shot与Few-Shot相同，除了对任务的自然语言描述外，只允许进行一次演示，如图所示在上图中。区分这两种方法的原因是，它与某些任务传达给人类的方式最接近。例如，当要求人类在计算机上生成数据集时人工服务（例如MechanicalTurk），通常会演示一项任务。相反，如果没有给出示例，有时很难传达任务的内容或格式。

Zero-shot与One-shot相同，不同之处在于不允许进行演示，并且仅向模型提供描述任务的自然语言指令。这种方法提供了最大的便利性，潜在的鲁棒性和避免了虚假的相关性（除非它们在大量的预训练数据中非常广泛地发生），但这也是最具挑战性的设置。在某些情况下，如果没有以前的示例，人甚至可能很难理解任务的格式，因此，在某些情况下，此设置“不公平”。例如，如果有人要求“制作200m的世界记录表”，此请求可能会很含糊，因为可能无法确切知道表格应采用的格式或应包含的格式（甚至在仔细澄清后，也很难准确地了解所需内容）。尽管如此，至少在某些设置上，零镜头最接近人类执行任务的方式–例如，在图2.1中的翻译示例中，人类可能仅会从文本指令中知道要做什么。

### 模型架构

我们使用与GPT-2相同的模型和体系结构，包括其中描述的修改后的初始化，预规范化和可逆的分词，不同之处在于，我们在层的各层使用交替的密集和局部条带稀疏模式transformer。为了研究ML性能对模型大小的依赖性，我们训练了8种不同大小的模型，范围从1.25亿个参数到1,750亿个参数，超过三个数量级，最后一个模型称为GPT-3。先前的工作建议，在具有足够的训练数据的情况下，验证损失的缩放比例应近似为大小函数的平稳幂定律；多种大小的训练模型使我们能够针对验证损失和下游语言任务测试这一假设。表2.1列出了8种模型的大小和体系结构。这里参数是可训练参数的总数，层是层的总数，dmodel是每个瓶颈层的单位数（我们始终将前馈层设为瓶颈层大小的四倍，d = 4 dmodel），而dhead是每个注意头。所有模型均使用nctx = 2048个令牌的上下文窗口。我们将模型沿着深度和宽度维度跨GPU划分，以最大程度地减少节点之间的数据传输。根据计算效率和跨GPU的模型布局中的负载平衡来选择每个模型的精确架构参数。先前的工作表明，验证损失对这些参数在相当宽的范围内不是很敏感。

### 数据集

语言模型的数据集已迅速扩展，最终达到了将近一万亿个单词的Common Crawl数据集。如此大的数据集足以训练我们最大的模型，而无需两次更新相同的序列。但是，我们发现，与经过精心挑选的数据集相比，未经过滤或经过轻微过滤的Common Crawl版本的质量往往较低。因此，我们采取了3个步骤来提高数据集的平均质量：（1）基于与一系列高质量参考语料库的相似性，下载并过滤了Common Crawl版本，（2）在文档级执行了重复数据删除，在数据集内和数据集之间，以防止冗余并保留我们保留的验证集的完整性，以作为过度拟合的精确度量;（3）我们还向训练组合中添加了已知的高质量参考语料库，以增强Common Crawl并增加其多样性。附录A中描述了前两点（Common Crawl的处理）。对于第三点，我们添加了多个精选的高质量数据集，包括WebText数据集的扩展版本，这些数据集是通过较长时间的爬网链接收集的时间，这是两种基于互联网的图书资料集（Books1和Books2）和英语Wikipedia。表2.2显示了我们在训练中使用的最终数据集。Common Crawl数据是从涵盖2016年至2019年的每月Common Crawl的41个分片中下载的，构成了过滤前的45TB压缩明文和过滤后的570GB，大致相当于4000亿字节对编码的词。请注意，在训练过程中，并非按大小对数据集进行采样，而是我们认为更高质量的数据集采样频率更高，因此Common Crawl和Books2数据集在训练过程中采样少于一次，而其他数据集则采样2-3次。这本质上是接受少量的过度拟合以换取更高质量的训练数据对于在大量互联网数据上进行预训练的语言模型，尤其是具有记忆大量内容的能力的大型模型，主要的方法论关注是通过在预训练期间无意中看到它们的测试或开发集，可能对下游任务造成污染。为了减少此类污染，我们进行了搜索并尝试消除与本文研究的所有基准测试的开发和测试集之间的任何重叠之处。不幸的是，筛选中的错误导致我们忽略了一些重叠之处，并且由于训练的代价是重新训练模型是不可行的。

## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**

具体结果参见论文

## 讨论与总结
1. GPT3方法的原理、解决问题、优势、不足是什么？
   GPT-3属于少样本学习语言模型，只需要少量标注数据，不管是 Zero-shot、One-shot 还是 Few-shot 都无需再进行微调。GPT-3 聚焦于更通用的 NLP 模型，主要目标是用更少的领域数据、且不经过精调步骤去解决问题。简单来说，GPT-3 是 GPT-2 的进化版，惊人的模型参数、训练数据和工作量以及结果证明了“大力出奇迹”的道理，扩展了NLP领域的想象力。

   GPT-3的可学习参数达到1750亿，是之前的非稀疏语言模型的10倍以上，并在few-shot的设置上测试它的性能。对于所有子任务，GPT-3不做任何的梯度更新或者是微调。GPT-3的模型和GPT-2一样。

2. 相比于GPT2，GPT3有哪些区别？

数据和模型都大了一倍。

3. GPT与BERT区别是什么？

BERT用的不是标准的语言模型，它用的是一个带掩码的语言模型，可以理解为它就是完形填空，完形填空就是给一个句子，把中间的一个词挖掉，预测中间的，就是说在预测的时候既能看到它之前的词，又能看到之后词，因此它可以使用Transformer的编码器，因为编码器上可以看到所有的信息。

使用编码器和解码器不是他们两个的主要区别，主要区别在于他们目标函数的选取，GPT用的是一个更难的，就是给前面一段话预测后面一个词，预测未来当然比完形填空要难。就是预测一个开放性结局你预测中间的一个状态难得多。这个也是GPT其实比BERT要差一些的一个原因。那么反过来讲如果这个模型真的能够预测未来的话。

