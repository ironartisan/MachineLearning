# 011

**英文名称：** Language Models are Few-Shot Learners

**中文名称：** 

**论文地址：** https://arxiv.org/pdf/2005.14165.pdf

**期刊/时间：** NeurIPS 2020

## 前置知识

## 摘要

- **问题是什么？**
- **我们要做什么？**
- **大概怎么做的**
- **实验效果**

最近的工作表明，在许多NLP任务和基准上，通过对大型文本语料库进行预训练，然后对特定的任务进行微调，可以获得巨大的收益。虽然在结构上通常是任务无关的，但这种方法仍然需要特定任务的微调数据集，包括几千或几万个例子。相比之下，人类通常只需从几个例子或简单的指令中就能完成一项新的语言任务，而目前的NLP系统在很大程度上仍难以做到这一点。

在这里，我们扩大语言模型的规模，大大改善了与任务无关的、少量的性能，有时甚至达到了与之前最先进的微调方法的竞争力。

具体来说，我们训练了GPT-3，一个具有1750亿个参数的自回归语言模型，比以前的任何非稀疏语言模型多出10倍，并测试了它在少样本情况下的表现。对于所有的任务，GPT-3的应用没有任何梯度更新或微调，纯粹通过与模型的文本互动来指定任务和少量演示。GPT-3在许多NLP数据集上取得了强大的性能，包括翻译、回答问题和cloze任务，以及一些需要即时推理或领域适应的任务，如解读单词、在句子中使用一个新词或进行3位数的算术。同时，我们也发现了一些数据集，在这些数据集中，GPT-3的小样本学习仍然很困难，还有一些数据集，GPT-3面临着与大型网络语料库训练有关的方法学问题。最后，我们发现，GPT-3可以生成人类评价者难以区分的新闻文章样本。我们讨论了这一发现和GPT-3总体上的更广泛的社会影响。

## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
- **承。相关工作**
- **转。相关工作的不足和转机**
- **合。本文工作**

近年来，在NLP系统中出现了预训练语言表征的趋势，并以越来越灵活和与任务无关的方式应用于下游转移。首先，使用词向量学习单层表征[MCCD13, PSM14]，并反馈给特定的任务架构，然后使用具有多层表征和上下文状态的RNN来形成更强大的表征[DL15, MBXS17, PNZtY18]（尽管仍然适用于特定的任务架构），最近，预训练的递归或transformer语言模型[VSP+17]被直接微调，完全消除对特定任务架构的需求[RNSS18, DCLT18, HR18]。

这最后一种范式在许多具有挑战性的NLP任务上取得了实质性的进展，如阅读理解、问题回答、文本嵌套等，并在新的架构和算法的基础上继续推进[RSR+19, LOG+19, YDY+19, LCG+19]。**然而，这种方法的一个主要限制是，虽然架构是任务无关的，但仍然需要特定任务的数据集和特定任务的微调：要在一个期望的任务上实现强大的性能，通常需要在该任务特定的数千到数十万个例子的数据集中进行微调**。消除这一限制是可取的，原因有几个。

首先，从实用的角度来看，每一项新的任务都需要一个大型的标记实例数据集，这限制了语言模型的适用性。因为存在着非常广泛的可能有用的语言任务，包括从纠正语法，到生成抽象概念的例子，再到批评一个短篇故事。对于许多这样的任务来说，收集一个大型的监督训练数据集是很困难的，尤其是当这个过程对于每个新任务必须为重复时。

其次，利用训练数据中的虚假关联的潜力从根本上说是随着模型的表现力和训练分布的狭窄程度而增长的。这可能会给预训练加微调范式带来问题，即模型被设计得很大，以便在预训练期间吸收信息，但随后在非常狭窄的任务分布中进行微调。例如，[HLW+20]观察到较大的模型不一定在分布外有更好的泛化。有证据表明，在这种范式下实现的泛化可能很差，因为模型对训练分布过于具体，在分布外的泛化效果不好[YdC+19, MPL19]。因此，微调模型在特定基准上的表现，即使名义上是人类水平，也可能夸大了基础任务的实际表现[GSL+18, NK19]。

第三，人类不需要大量的监督数据集来学习大多数语言任务--自然语言的简短指令（例如 "请告诉我这个句子描述的是快乐的事情还是悲伤的事情"）或最多极少的示范（例如 "这里有两个人们表现勇敢的例子；请给出第三个勇敢的例子"）往往就是足以使人类在执行一项新任务时至少达到合理的能力水平。除了指出我们目前的NLP技术在概念上的局限性外，这种适应性还有实际的好处--它允许人类在许多任务和技能之间无缝混合或切换，例如在漫长的对话中进行加法。为了发挥广泛的作用，我们希望有一天我们的NLP系统也能有这样的流畅性和通用性。

解决这些问题的一个潜在途径是元学习1--在语言模型的背景下，这意味着模型在训练时发展出一套广泛的技能和模式识别能力，然后在推理时使用这些能力来快速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC+19]试图通过我们所说的 "语境学习 "来做到这一点，使用预训练的语言模型的文本输入作为任务规范的形式：该模型以自然语言指令和/或任务的几个示范为条件，然后期望通过预测接下来的内容来完成任务的进一步实例。

![20221105165446-2022-11-05-16-54-48](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20221105165446-2022-11-05-16-54-48.png)

虽然它已经显示出一些初步的希望，但这种方法所取得的结果仍然远远不如微调--例如[RWC+19]在自然问题上只取得了4%的成绩，甚至其55 F1 CoQa的结果现在也比技术水平落后35分以上。元学习显然需要大量的改进，以便作为解决语言任务的实用方法。

语言建模的另一个最新趋势可能提供了一个前进的方向。近年来，Transformer语言模型的容量大幅增加，从1亿个参数[RNSS18]，到3亿个参数[DCLT18]，到15亿个参数[RWC+19]，到80亿个参数[SPP+19]，110亿个参数[RSR+19]，最后到170亿个参数[Tur20]。每一次增加都带来了文本合成和/或下游NLP任务的改进，而且有证据表明，与许多下游任务有很好关联的对数损失，随着规模的扩大也有平滑的改进趋势[KMH+20]。由于语境学习涉及到在模型的参数范围内吸收许多技能和任务，因此，语境学习能力可能会随着规模的扩大而显示出类似的强劲收益，这是可信的。

在本文中，我们通过训练一个1750亿个参数的自回归语言模型（我们称之为GPT-3）来测试这一假设，并测量其语境学习能力。具体来说，我们在二十多个NLP数据集上评估GPT-3，以及几个旨在测试快速适应不太可能直接包含在训练集中的任务的新任务。对于每个任务，我们在3种条件下评估GPT-3：(a) "few-shot学习"，或语境学习，我们允许尽可能多的演示，以适应模型的上下文窗口（通常是10至100），(b) "One-shot学习"，我们只允许一次演示，和(c) "zero-shot"学习，不允许演示，只给模型一个自然语言的指令。原则上，GPT-3也可以在传统的微调设置中进行评估，但我们将此留给未来的工作。

图1.2说明了我们研究的条件，并显示了对一个简单任务的few-shot学习，该任务要求模型从一个词中去除不相干的符号。模型的性能随着自然语言任务描述的增加而提高，也随着模型上下文中的例子数量K的增加而提高。虽然这种情况下的结果特别引人注目，但模型规模和上下文中的例子数量的一般趋势在我们研究的大多数任务中都适用。我们强调，这些 "学习 "曲线不涉及梯度更新或微调，只是作为条件的示范数量增加。

![20221105172657-2022-11-05-17-26-57](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20221105172657-2022-11-05-17-26-57.png)

总的来说，在NLP任务中，GPT-3在zero-shot和one-shot设置中取得了令人满意的结果，在few-shot设置中有时与最先进的技术竞争，甚至有时超过最先进的技术(尽管最先进技术是由微调的模型保持的)。例如，GPT-3在zero-shot条件下CoQA达到81.5 F1，在one-shot条件下CoQA达到84.0 F1，在few-shot条件下达到85.0 F1。类似地，GPT-3在TriviaQA上的zero-shot命中率为64.3%，在one-shot命中率为68.0%，在few-shot命中率为71.2%，相对于在相同设置下运行的微调模型，后者是最先进的。

与此同时，我们也发现了一些任务，即使在GPT-3的规模下，few-shot的性能挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优缺点(包括这些局限性)进行广泛的描述，我们希望促进对语言模型中的few-shot学习的研究，并让人们注意到最需要进步的地方。

与此同时，我们也发现了一些任务，即使在GPT-3的规模下，few-shot的性能挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优缺点(包括这些局限性)进行广泛的描述，我们希望促进对语言模型中的few-shot学习的研究，并让人们注意到最需要进步的地方。

在图1.3中可以看到总体结果的启发式意义，它聚集了各种任务(尽管它本身不应该被视为严格或有意义的基准)。我们还对“数据污染”进行了系统的研究，这是一个日益严重的问题，当训练高容量模型时，如Common Crawl数据集，它可能包含来自测试数据集的内容，只是因为这样的内容经常存在于web上。在这篇论文中，我们开发了测量数据污染和量化其扭曲影响的系统工具。尽管我们发现数据污染对GPT-3在大多数数据集上的性能影响极小，但我们确实发现一些数据集上的结果可能是夸大的，我们要么不报告这些数据集上的结果，要么根据严重程度用星号标记它们。

除了以上所有这些，我们还训练了一系列较小的模型(从1.25亿个参数到130亿个参数)，以便比较它们在zero, one 及few-shot设置下的性能。总的来说，对于大多数任务，我们发现在所有三种设置中模型容量的伸缩相对平稳;一个值得注意的模式是，zero, one 及few-shot表现之间的差距通常随着模型容量的增加而增大，这可能表明较大的模型是更熟练的元学习者。

最后，鉴于GPT-3显示的广泛能力，我们讨论了对偏见、公平和更广泛的社会影响的关注，并试图对GPT-3在这方面的特点进行初步分析。

## 相关工作

**主要介绍背景知识。**

## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**



## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**


## 讨论与总结
1. GPT3方法的原理、解决问题、优势、不足是什么？
   GPT-3属于少样本学习语言模型，只需要少量标注数据，不管是 Zero-shot、One-shot 还是 Few-shot 都无需再进行微调。GPT-3 聚焦于更通用的 NLP 模型，主要目标是用更少的领域数据、且不经过精调步骤去解决问题。简单来说，GPT-3 是 GPT-2 的进化版，惊人的模型参数、训练数据和工作量以及结果证明了“大力出奇迹”的道理，扩展了NLP领域的想象力。

   GPT-3的可学习参数达到1750亿，是之前的非稀疏语言模型的10倍以上，并在few-shot的设置上测试它的性能。对于所有子任务，GPT-3不做任何的梯度更新或者是微调。GPT-3的模型和GPT-2一样。

2. 相比于GPT2，GPT3有哪些区别？

数据和模型都大了一倍。

3. GPT与BERT区别是什么？

BERT用的不是标准的语言模型，它用的是一个带掩码的语言模型，可以理解为它就是完形填空，完形填空就是给一个句子，把中间的一个词挖掉，预测中间的，就是说在预测的时候既能看到它之前的词，又能看到之后词，因此它可以使用Transformer的编码器，因为编码器上可以看到所有的信息。

使用编码器和解码器不是他们两个的主要区别，主要区别在于他们目标函数的选取，GPT用的是一个更难的，就是给前面一段话预测后面一个词，预测未来当然比完形填空要难。就是预测一个开放性结局你预测中间的一个状态难得多。这个也是GPT其实比BERT要差一些的一个原因。那么反过来讲如果这个模型真的能够预测未来的话。

