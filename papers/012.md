# 012

**英文名称：** XLNet: Generalized Autoregressive Pretraining for Language Understanding

**中文名称：** 

**论文地址：** https://arxiv.org/pdf/1906.08237.pdf

**期刊/时间：** 2020

## 前置知识

## 摘要

- **问题是什么？**
- **我们要做什么？**
- **大概怎么做的**
- **实验效果**

与基于自回归语言建模的预训练方法相比，基于去噪自编码的预训练方法（BERT等）具有双向上下文建模的能力，能取得更好的性能。然而，BERT依赖于用掩码破坏输入，忽略了掩码位置之间的依赖性，并遭受预训练-微调差异的困扰。考虑到这些优点和缺点，我们提出了一种广义自回归预训练方法XLNet，它(1)通过在所有分解顺序的排列上最大化期望似然来实现双向上下文学习;(2)通过其自回归克服了BERT的局限性。此外，XLNet将Transformer-XL(最先进的自回归模型)的思想集成到预训练中。经验上，在类似的实验设置下，XLNet在20个任务上优于BERT，通常有很大的优势，包括问题回答、自然语言推理、情感分析和文档排名。


## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
- **承。相关工作**
- **转。相关工作的不足和转机**
- **合。本文工作**

无监督表示学习在自然语言处理领域非常成功。通常，这些方法首先在大规模无标记文本语料库上对神经网络进行预训练，然后对下游任务的模型或表示进行微调。在这种共享的高水平思想下，不同的无监督预训练目标在文献中进行了探讨。其中，自回归(AR)语言建模和自动编码(AE)是两个最成功的预训练目标。

AR语言建模试图通过自回归模型估计文本语料库的概率分布。具体来说，给定一个文本序列$\mathbf{x}=\left(x_1, \cdots, x_T\right)$， AR语言建模将可能性分解为正向乘积$p(\mathbf{x})=\prod_{t=1}^T p\left(x_t \mid \mathbf{x}_{<t}\right)$或反向乘积$p(\mathbf{x})=\prod_{t=1}^T p\left(x_t \mid \mathbf{x}_{>t}\right)$。一个参数模型(例如神经网络)被训练来模拟每个条件分布。由于AR语言模型仅被训练为对单向上下文(向前或向后)进行编码，因此它在建模深度双向上下文时并不有效。相反，下游语言理解任务往往需要双向上下文信息。这就导致了AR语言建模和有效的预训练之间的差距。

相比之下，基于AE的预训练不执行显式密度估计，而是旨在从损坏的输入中重建原始数据。一个显著的例子是BERT，它是最先进的训练前方法。给定输入的令牌序列，用一个特殊的符号[MASK]替换部分令牌，训练模型从损坏的版本中恢复原始令牌。由于密度估计不是目标的一部分，BERT允许用双向的内容来重建。作为一个直接的好处，这消除了AR语言建模中前面提到的双向信息差距，从而提高了性能。然而，BERT在预训练中使用的[MASK]等人工符号在微调时真实数据不存在，导致预训练与微调之间存在差异。此外，由于预测的标记被屏蔽在输入中，BERT不能像AR语言建模中那样使用乘积规则建模联合概率。换句话说，BERT假设预测的标记是相互独立的，给出了未掩码的标记，这是过度简化的，因为高阶、远程依赖在自然语言中普遍存在。

面对现有语言预训练目标的优缺点，在本研究中，我们提出了一种广义自回归方法XLNet，该方法充分利用了增强现实语言建模和AE的优点，同时避免了它们的局限性。

- 首先，与传统AR模型使用固定的正向或向后分解顺序不同，XLNet最大化了序列的期望对数似然，w.r.t.分解顺序的所有可能排列。多亏了置换操作，每个位置的上下文可以由左右两种标记组成。在期望中，每个位置学习利用所有位置的上下文信息，即捕捉双向上下文。

- 其次，XLNet作为一种广义AR语言模型，不依赖于数据损坏。因此，XLNet不受BERT所受的预训练-微调差异的影响。同时，自回归目标还提供了一种自然的方法，利用乘积规则对预测标记的联合概率进行因式分解，消除了BERT中所做的独立性假设。

除了一个新的预训练目标，XLNet改进了训练前的体系结构设计。

- 受AR语言建模最新进展的启发，XLNet将Transformer-XL[9]的片段递推机制和相关编码方案集成到预训练中，这在经验上提高了性能，特别是涉及较长文本序列的任务。

- 直接将Transformer(-XL)体系结构应用到基于置换的语言建模中是行不通的，因为分解顺序是任意的，目标是模糊的。作为一种解决方案，我们建议重新参数化Transformer(-XL)网络以消除模糊性。

从经验上看，在类似的实验设置下，XLNet在一系列问题上的表现一直优于BERT，这些问题包括GLUE语言理解任务、阅读理解任务(如SQuAD和RACE)、文本分类任务(如Yelp和IMDB)以及ClueWeb09-B文档排序任务。
## 相关工作

**主要介绍背景知识。**

基于排列的AR建模的思想已经在[32,12]中进行了探讨，但有几个关键的区别。首先，之前的模型旨在通过在模型中添加“无序”归纳偏差来改进密度估计，而XLNet则是通过使AR语言模型能够学习双向上下文来实现的。从技术上讲，为了构建有效的目标感知预测分布，XLNet通过双流注意将目标位置合并到隐藏状态中，而之前基于排列的AR模型依赖于MLP架构固有的隐式位置感知。最后，对于无序NADE和XLNet，我们想强调的是，“无序”并不意味着输入序列可以随机排列，而是模型允许分布的不同分解顺序。

另一个相关的想法是在文本生成的上下文中执行自回归去噪，尽管它只考虑固定的顺序。

## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**



## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**


## 讨论与总结
1. 与BERT及GPT的区别是什么？
   - XLNET和Bert的区别：与AR语言模型不同，BERT被分类为自动编码器（AE）语言模型。AE语言模型旨在从损坏的输入中重建原始数据。像BERT，原始文本中的一定比例token会被mask掉，然后训练模型从被mask掉的文本中重现原来的文本。XLNet本质上是用自回归语言模型来同时编码双向语义信息的思路，可以克服BERT存在的依赖缺失和训练/微调不一致的问题。
   - 
2. XLNet的创新点和不足是什么？
   - 创新点：
     - XLNET是基于自回模型上的，但是它不只是向前或向后，而是双方的排列来获取依赖信息。避免单向信息流。
     - 作为一种广义的AR语言模型，XLNet不依赖于数据破坏。避免mask丢失信息。避免与训练与微调的差异弊端。
     - 融合了transformerXL的方法
    
   - 不足：

   

