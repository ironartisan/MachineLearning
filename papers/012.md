# 012

**英文名称：** XLNet: Generalized Autoregressive Pretraining for Language Understanding

**中文名称：** 

**论文地址：** https://arxiv.org/pdf/1906.08237.pdf

**期刊/时间：** 2020

## 前置知识

## 摘要

- **问题是什么？**
- **我们要做什么？**
- **大概怎么做的**
- **实验效果**

与基于自回归语言建模的预训练方法相比，基于去噪自编码的预训练方法（BERT等）具有双向上下文建模的能力，能取得更好的性能。然而，BERT依赖于用掩码破坏输入，忽略了掩码位置之间的依赖性，并遭受预训练-微调差异的困扰。考虑到这些优点和缺点，我们提出了一种广义自回归预训练方法XLNet，它(1)通过在所有分解顺序的排列上最大化期望似然来实现双向上下文学习;(2)通过其自回归克服了BERT的局限性。此外，XLNet将Transformer-XL(最先进的自回归模型)的思想集成到预训练中。经验上，在类似的实验设置下，XLNet在20个任务上优于BERT，通常有很大的优势，包括问题回答、自然语言推理、情感分析和文档排名。


## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
- **承。相关工作**
- **转。相关工作的不足和转机**
- **合。本文工作**



## 相关工作

**主要介绍背景知识。**

## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**



## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**


## 讨论与总结
1. 与BERT及GPT的区别是什么？
   - XLNET和Bert的区别：XLNet本质上是用自回归语言模型来同时编码双向语义信息的思路，可以克服BERT存在的依赖缺失和训练/微调不一致的问题。
   - 
2. XLNet的创新点和不足是什么？
   - 创新点：
    
   - 不足：
3. XLNET的原理是什么？
   - xlnet是基于自回模型上的，但是它不只是向前或向后，而是双方的排列来获取依赖信息。避免单向信息流。
   - 作为一种广义的AR语言模型，XLNet不依赖于数据破坏。避免mask丢失信息。避免与训练与微调的差异弊端。
   - 融合了transformerXL的方法

