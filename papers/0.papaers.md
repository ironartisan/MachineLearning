# 论文阅读笔记


## 001

**英文名称：** Efficient Estimation of Word Representations in Vector Space

**中文名称：** 向量空间中词表示的有效估计

**期刊/时间：**

**论文地址：** https://arxiv.org/abs/1301.3781

### 痛点及现状

- 简单的技术在很多任务中都具有限制。
- 专注于神经网络学习的单词的分布式表示，因为之前已经证明它们在保持单词之间的线性规律性方面的表现明显优于LSA，此外，LDA 在大型数据集上的计算成本非常高。
- 神经网络语言模型可以通过两步成功训练：首先，使用简单模型学习连续词向量，然后在这些分布式词的表示之上训练N-gram。
### 方法与创新

- 尝试通过开发新的模型架构来最大限度地提高这些向量运算的准确性，以保持单词之间的线性规律。提出了两种新颖的模型架构，用于从非常大的数据集中计算单词的连续向量表示。
  - CBOW
    - 类似于前馈 NNLM，其中去除了非线性隐藏层，所有单词共享投影层（不仅仅是投影矩阵）；因此，所有单词都被投影到相同的位置（它们的向量被平均）。
  - Skip-gram
    - 中心词预测输出上下文
    - 类似于 CBOW，但它不是根据上下文预测当前单词，而是尝试根据同一句子中的另一个单词最大化对单词的分类。
- 设计了一个新的综合测试集来测量句法和语义规则，并表明可以以高精度学习许多这样的规则。
- 讨论了词向量的维度和训练数据的数量对训练时间和准确性的影响。


### 实验及结论

- 我们观察到以更低的计算成本显着提高了准确性，即从 16 亿个单词数据集中学习高质量的单词向量只需不到一天的时间
- 我们证明这些向量在我们的测试集上提供了最先进的性能，用于测量句法和语义词的相似性。
- 有些问题可以通过对单词向量表示的代数运算来回答，训练得到的向量可以回答词之间微妙的语义关系。

**数据集：**

- 定义了一个包含五种语义问题和九种句法问题的综合测试集。总体而言，有 8869 个语义问题和 10675 个句法问题。
- SemEval-2012 Task 2 



## 002

**英文名称：** Distributed Representations of Words and Phrases and their Compositionality
**中文名称：** 词和短语的分布式表示及其组合性
**论文地址：** https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html

### 痛点及现状

词表示容易受到限制，因为他们不能通过简单词的组合的方式来表示某个单词的惯用短语。

### 方法与创新

- Skip-gram可以通过对词向量表示,使用基本的数学运算来获得不明显的语言理解程度。
- 对频繁词二次采样，获得了显著的加速和更加规则的词表示。
- 描述了一种称为负采样的方法，用于替换分层Softmax。
- 提出了一种在文本中查找短语的简单方法，并表明学习数百万个短语的良好向量表示是可能的。


### 实验及结论

- 另一个贡献是负采样算法，这是一种极其简单的训练方法，可以学习准确的表示，尤其是对于频繁出现的单词。


## 003

**英文名称：** Distributed representations of sentences and docments
**中文名称：** 句子和文档的分布式表示
**论文地址：** http://arxiv.org/abs/1405.4053
**代码地址：** https://github.com/JonathanRaiman/PVDM

### 痛点及现状

- BOW存在数据稀疏性和高维性的问题，有两个主要缺点：
  - 丢失了词的顺序
  - 忽略了词的语义，或者单词之间的距离

之前学习向量表示的方法存在缺陷：
- 词向量的加权平均，它以与标准词袋模型相同的方式丢失词序。
- 使用解析树组合词向量，已被证明仅适用于句子，因为它依赖于解析。


### 方法与创新

- 虽然段落向量在段落之间是唯一的，但词向量是共享的。在预测时，通过固定词向量并训练新的段落向量直到收敛来推断段落向量。
- 与之前的一些方法不同，它是通用的并且适用于任何长度的文本：句子、段落和文档。它不需要对词权函数进行特定于任务的调整，也不依赖于解析树。
- **与word2vec不同，将paragraph和word平均或结合来预测下个词**
- Paragraph向量解决了词袋模型的缺点。
  - 继续了word2vec的重要特性：词的语义
  - 将词的顺序考虑在内
- 之前的方法受限于句子，我们的方法不需要解析，可以生成包含很多句子的长文档表示，相比更加通用。
### 实验及结论

分别从两方面进行实验验证：
- Sentiment Analysis
  - 在Stanford sentiment treebank数据集和IMDB上表现SOTA
- Information Retrieval
  - 在新建数据集上表现SOTA



## 004

**英文名称：**NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE
**中文名称：**基于联合学习对齐和翻译的神经机器翻译
**论文地址：** http://arxiv.org/abs/1409.0473
**期刊/时间：**ICLR 2015

### 痛点及现状

编码器-解码器方法的一个潜在***\*问题是神经网络需要能够将源句子的所有必要信息压缩到一个固定长度的向量中。\****，使得神经网络难以处理长句子，随着输入句子长度的增加，编码器-解码器的性能会逐渐下降。

### 方法与创新

![20220916223424-2022-09-16-22-34-25](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20220916223424-2022-09-16-22-34-25.png)

- 没有将输入编码到固定维度向量，将输入的句子编码成**变长向量序列**
- 提出了一种模型RNNsearch，包括一个双向RNN作为编码器和一个解码器
- 使用BiRNN结构同时对当前单词的前面和后面的信息进行编码
- 解码器中引入**注意机制**，对输入的隐藏状态求权重，使得解码器可以相应地有选择地检索。

### 实验及结论

**数据集：**  ACL WMT ’14

- RNNsearch比传统的encoder-decoder模型表现更好，在长句子上更具有鲁棒性。
- 翻译性能与现有的基于短语的统计机器翻译能力相当。




