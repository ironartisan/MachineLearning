# 010 GPT2

**英文名称：** Language Models are Unsupervised Multitask Learners

**论文地址：** https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

**期刊/时间：** 2019



## 摘要

- **问题是什么？**
- **本文要做什么？**
- **大概怎么做的**
- **实验效果**

自然语言处理任务，如问题回答、机器翻译、阅读理解和总结，通常在特定任务的数据集上进行监督学习。本文证明，当在一个名为WebText的数百万网页的新数据集上训练时，语言模型开始在没有任何明确监督的情况下学习这些任务。当以文档加问题为条件时，语言模型产生的答案在CoQA数据集上达到了55个F1--匹配或超过了4个基线系统中的3个，而没有使用127,000多个训练例子。

语言模型的容量对zero-shot任务转移的成功至关重要，增加语言模型可以在不同的任务中以对数线性的方式提高性能。本文最大的模型GPT-2是一个1.5B参数的转化器，在zero-shot的情况下，在8个测试的语言建模数据集中的7个取得了最先进的结果，但仍然不适合WebText。该模型的样本反映了这些改进，并包含连贯的文本段落。这些发现为建立语言处理系统提供了一条有希望的道路，该系统可以从自然发生的示范中学习执行任务。

## 介绍

按照起承转合的思想阅读。
- **起。做的哪方面工作？**
- **承。相关工作**
- **转。相关工作的不足和转机**
- **合。本文工作**

目前的监督学习下的机器学习模型在大数据量与高参数量的情况在表现都很不错，但是至今这些模型在数据分布和任务规范程度有轻微改动时都很脆弱、敏感，当前的这些模型其实更适合被描述为一个狭隘的专家，而不是能干的通才，而本文更想去构建一个更加通用的系统能胜任更多的任务，甚至不需要去手动的打标签做微调。

目前主流的机器学习模型都是在指定的任务上去用一部分数据来训练模型，再用一部分不相同但同分布的数据来测试其性能。这样的模式在特定场景效果的确不错，但是对于字幕或者说阅读理解、图像分类这样的任务来说，输入的多样性和不确定性就会把缺点给暴露出来。

本文认为，目前普遍采用的单一领域的数据来训练单一模型是该问题的罪魁祸首，如果要构建一个泛化能力更强的模型，需要在更广泛的任务和领域上进行训练。目前许多测试基准都已经提出了该理念，例如GLUE、decaNLP。

多任务学习对于提升模型性能来说是很有用的，但是多任务学习在NLP领域还是新生儿。从元学习的角度来看，每个数据对都是从数据集和目标的分布中抽样的单个训练示例。目前的机器学习系统需要成百上千的数据去拟合出更好的函数，这也表明多任务学习需要更多的数据才能达到好的效果，但是就目前的技术来说，继续扩大数据集的规模是很困难的，这也促使本文去探索更多方法来提升多任务学习的效果。

目前效果最好的形式就是预训练模型并采用监督学习做微调的模式，该模式已经有了很长的历史，但是依旧是未来的趋势，起初都是采用词向量来作为特定任务的输入，到后来采用循环神经网络的上下文信息，最近的研究也表明，特定任务的体系结构其实并不是必须的，多个self-attention模块就已经能够满足目前的需要。

对于一些特定的任务可能需要采用监督学习，当只有少量或者没有打过标签的数据的时，另一项研究表明语言模型可通过执行其他一些特定的任务来进行训练，例如常识推理，情感分析。

在本篇论文中，本文会把多任务学习和非监督学习联系起来，并介绍一种更加具有趋向性的方法。本文展示的语言模型能在没有训练样本（没有任何参数和结构的修改）的情况下执行一些下游任务，并且在零样本的情况下泛化性能更强，在一些任务上本文也取得了业界最佳的效果


## 相关工作

**主要介绍背景知识。**

## 方法

- **简要地重复问题**
- **解决思路**
- **必要的形式化定义**
- **具体模型**

原则上来说，语言模型不需要对输出哪一个符号做明确的监督学习。虽然监督学习和非监督学习的目标是相同的，但是监督学习只能在子集上进行评估，非监督学习的全局最优解也是监督学习的全局最优解，因此是否能够在训练的过程中让非监督学习的目标能够收敛成了最大的问题。初期的实验表明足够大的语言模型是能够执行多任务的，但是其训练速度明显比监督学习的方法慢很多。


Input Representation采用BPE（Byte Pair Encoding），虽命名为Byte，通常是在Unicode序列上做的方法，且在字节级别上进行合并但是限制合并不同类型的字符以避免类似dog. dog! dog?的出现。这种表示方式能够结合词级别语言模型的优点和字节级别的泛化性能，更加灵活。简单来说就是不用词向量而用UTF-8的二进制表示，这种表示方法适合插值。对于一些常见词汇，如dog，由于字典槽位有限，BPE很难区别dog.dog?dog!之间的情感差别。为了避免陷入局部最优，本文严禁不同种类byte sequence的拼接。最终模型结合了词级别的语言模型的优点和字节级别的泛化能力。

模型在GPT的基础做了一点小改动，例如Layer normalization前移到每个子模块之前，最后一层自注意力模块多加一层Layer normalization，初始化策略，增加context大小等。

## 实验

- **数据集和实验设置**
- **主实验，提供详尽的实验分析**

模型验证的第一步是零样本任务，本文对WebText训练的语言模型在在一些基本任务零样本的情况下的效果很感兴趣。本文的模型是在字节级别做的处理，因此并不需要做预处理或者说是分词，本文能够在任何语言模型的基准上进行评估。语言建模数据集上的结果通常以每个标准单元(字符、字节或者词)的平均负样本对数概率的比例或指数作为评判标准。本文的评判标准是：对数概率除以标准节点的个数。对于这些数据集来说，WebText语言模型能明显的测出out-of分布，它能找出没有联系的标点符号、缩进、打乱的句子，甚至，不过这个在WebText中出现的次数极少，400亿字节中只出现了26次。下图是本文的模型在各项任务的表现，本文采用了invertible de-tokenizers并尽可能的把人为的预处理过程移除了。

![20230109205129](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20230109205129.png)

正是由于采用了invertible de-tokenizers，本文仍然可以计算数据集的对数概率值，该值可以被认为是一种简单的域自适应结果。本文发现GPT-2在invertible de-tokenizers作用下PPL的值降低了2.5-5个点。

WebText语言模型跨越了不同的领域与数据集，在零样本的情况下8个数据集测试情况中有7个取得了最好的结果，如果在WiliText-2这样的只有1、2百万训练数据的数据集上微调的话效果会有巨大的提升，对于有长期依赖关系的数据集也有很大的提升。但是本文的模型在One Billion Word Benchmark上效果并不如之前做的好，可能是由于本文预训练阶段数据集太大，打乱了其long-range的结构。

其他实验结果请参考原文

## 讨论与总结

当一个大型的语言模型在一个足够大和多样化的数据集上训练时，它能够在许多领域和数据集上表现良好。GPT-2在8个测试的语言建模数据集中的7个上达到了最先进的性能。该模型在zero-shots环境下能够执行的任务的多样性表明，为最大限度地提高足够多的文本语料的可能性而训练的高容量模型开始学习如何执行大量的任务，而不需要明确的监督。

### 问题

1. 论文主要关注于解决零样本问题，它的解决思路是什么？
   GPT2使用多任务学习的方式，它的学习目标是使用无监督的预训练模型做有监督的任务。作者认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。
2. 与GPT相比，GPT2的改进是什么？
    相比于GPT，GPT2的模型做了简单调整。GPT-2的主要改进点：多任务预训练+超大数据集+超大规模模型
    - 在Pretrain部分基本与GPT方法相同，去掉了fine-tuning层，不再针对不同任务分别进行微调建模，而是不定义这个模型应该做什么任务，模型会自动识别出来需要做什么任务。
    - 增加数据集，并对数据进行了质量筛选，而且添加了更宽泛的数据。
    - 增加网络参数。GPT是12层的transformer，BERT最深是24层的transformer，GPT-2将Transformer堆叠的层数设置为48层，隐层的维度为1600，参数量更是达到了15亿。
    - 调整网络结构。例如Layer normalization前移到每个子模块之前，最后一层自注意力模块多加一层Layer normalization，初始化策略，增加context大小等。
3. 与Bert的模型区别是什么？
   - **语言模型：** Bert和GPT-2虽然都采用transformer，但是Bert使用的是transformer的encoder，即：Self Attention，是双向的语言模型；而GPT-2用的是transformer中去掉中间Encoder-Decoder Attention层的decoder，即：Masked Self Attention，是单向语言模型。
   - **结构：** Bert是pre-training + fine-tuning的结构；而GPT-2只有pre-training。
   - **输入向量：** GPT-2是token embedding + position embedding；Bert是 token embedding + position embedding + segment embedding。
   - **参数量：**Bert是3亿参数量；而GPT-2是15亿参数量。
   - Bert引入Masked LM和Next Sentence Prediction；而GPT-2只是单纯的用单向语言模型进行训练。

4. 论文的创新点与不足是什么？
    GPT2最主要的贡献是探索了更大规模的模型在ZERO-SHOT的情况下的表现，没有使用任何微调，仅靠预训练+提示+预测就在8/9个任务里达到了SOTA。
    但LM训练大概需要三方面，模型结构，目标函数和数据。和GPT相比，GPT2模型结构和目标函数几乎是一样的，只有数据不一样，这样文章的创新就显得不足。模型大小是BERT large的五倍参数量，两倍的深度。训练成本高。